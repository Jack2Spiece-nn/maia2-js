{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configurations:\n",
      "\tseed: 42\n",
      "\tnum_workers: 0\n",
      "\tverbose: 1\n",
      "\tmax_ply: 300\n",
      "\tclock_threshold: 30\n",
      "\tuse_clock_filter: True\n",
      "\tchunk_size: 20000\n",
      "\tfrom_checkpoint: None\n",
      "\tmax_games_per_elo_range: 20\n",
      "\tbatch_size: 2048\n",
      "\tfirst_n_moves: 10\n",
      "\tlast_n_moves: 10\n",
      "\tdim_cnn: 256\n",
      "\tdim_vit: 1024\n",
      "\tnum_blocks_cnn: 5\n",
      "\tnum_blocks_vit: 2\n",
      "\tinput_channels: 18\n",
      "\tvit_length: 8\n",
      "\telo_dim: 128\n",
      "\tside_info: True\n",
      "\tvalue: True\n",
      "\tmax_depth: 1\n"
     ]
    }
   ],
   "source": [
    "from eval_maia2 import parse_args\n",
    "from utils import seed_everything, get_all_possible_moves, create_elo_dict\n",
    "from net import MAIA2Model\n",
    "\n",
    "cfg = parse_args(args=\"\")\n",
    "print(\"Configurations:\", flush=True)\n",
    "for arg in vars(cfg):\n",
    "\tprint(f\"\\t{arg}: {getattr(cfg, arg)}\", flush=True)\n",
    "seed_everything(cfg.seed)\n",
    "\n",
    "all_moves = get_all_possible_moves()\n",
    "all_moves_dict = {move: i for i, move in enumerate(all_moves)}\n",
    "# elo_dict = create_elo_dict()\n",
    "elo_dict = create_elo_dict()\n",
    "\n",
    "# Load MAIA2 model from checkpoint\n",
    "model = MAIA2Model(len(all_moves), elo_dict, cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/grace/u/dhkim2810/envs/maia/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "WARNING:root:Please consider to run pre-processing before quantization. Refer to example: https://github.com/microsoft/onnxruntime-inference-examples/blob/main/quantization/image_classification/cpu/ReadMe.md \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantized model saved at checkpoints/onnx/maia2_ft_quant.onnx\n"
     ]
    }
   ],
   "source": [
    "# Load model from checkpoint\n",
    "import torch\n",
    "import torch.onnx\n",
    "import onnx\n",
    "from onnxruntime.quantization import quantize_dynamic, QuantType\n",
    "\n",
    "checkpoint = torch.load(\"tmp/0.0001_8192_1e-05_FT_All/epoch_2_2023-03.pgn.pt\", map_location=\"cpu\")\n",
    "state_dict = {k.replace(\"module.\", \"\"): v for k, v in checkpoint[\"model_state_dict\"].items()}\n",
    "model.load_state_dict(state_dict)\n",
    "model.eval()\n",
    "\n",
    "# Export model to ONNX\n",
    "# (Pdb) boards.shape                                                                                                                               │\n",
    "# torch.Size([2048, 18, 8, 8])                                                                                                                     │\n",
    "# (Pdb) elos_self.shape                                                                                                                            │\n",
    "# torch.Size([2048])                                                                                                                               │\n",
    "# (Pdb) elos_oppo.shape                                                                                                                            │\n",
    "# torch.Size([2048])\n",
    "# Create dummy inputs for the model\n",
    "dummy_input1 = torch.randn(1, 18, 8, 8)\n",
    "dummy_input2 = torch.Tensor([0]).long()\n",
    "dummy_input3 = torch.Tensor([0]).long()\n",
    "\n",
    "# Define the file path for the ONNX model\n",
    "onnx_model_path = \"checkpoints/onnx/maia2_ft.onnx\"\n",
    "quantized_model_path = \"checkpoints/onnx/maia2_ft_quant.onnx\"\n",
    "\n",
    "# Export the model\n",
    "torch.onnx.export(\n",
    "    model,                 # Model being run\n",
    "    (dummy_input1, dummy_input2, dummy_input3),  # Model inputs as a tuple\n",
    "    onnx_model_path,       # Where to save the model (can be a file or file-like object)\n",
    "    export_params=True,    # Store the trained parameter weights inside the model file\n",
    "    opset_version=11,      # The ONNX version to export the model to\n",
    "    do_constant_folding=True,  # Whether to execute constant folding for optimization\n",
    "    input_names=['boards', 'elo_self', 'elo_oppo'],  # The model's input names\n",
    "    output_names=['logits_maia', 'logits_side_info', 'logits_value'],  # The model's output names\n",
    "    dynamic_axes={\n",
    "        'boards': {0: 'batch_size'}, \n",
    "        'elo_self': {0: 'batch_size'}, \n",
    "        'elo_oppo': {0: 'batch_size'}, \n",
    "        'logits_maia': {0: 'batch_size'},\n",
    "        'logits_side_info': {0: 'batch_size'},\n",
    "        'logits_value': {0: 'batch_size'},\n",
    "    }  # Variable length axes\n",
    ")\n",
    "\n",
    "print(f\"Model has been converted to ONNX and saved at {onnx_model_path}\")\n",
    "\n",
    "# Load the ONNX model\n",
    "onnx_model = onnx.load(onnx_model_path)\n",
    "\n",
    "# Apply dynamic quantization\n",
    "quantized_model = quantize_dynamic(\n",
    "    onnx_model_path,              # Path to the model to quantize\n",
    "    quantized_model_path,         # Path to save the quantized model\n",
    "    weight_type=QuantType.QUInt8  # Quantize weights to 8-bit unsigned integers\n",
    ")\n",
    "\n",
    "print(f\"Quantized model saved at {quantized_model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "import onnxruntime as ort\n",
    "import numpy as np\n",
    "import tqdm\n",
    "\n",
    "# Load ONNX model\n",
    "# onnx_model_path = \"checkpoints/onnx/maia2_ft.onnx\"\n",
    "onnx_model_path = \"checkpoints/onnx/maia2_ft_quant.onnx\"\n",
    "onnx_model = onnx.load(onnx_model_path)\n",
    "onnx.checker.check_model(onnx_model)\n",
    "\n",
    "# Initialize ONNX runtime session\n",
    "ort_session = ort.InferenceSession(onnx_model_path)\n",
    "\n",
    "def to_numpy(tensor):\n",
    "    return tensor.detach().cpu().numpy() if tensor.requires_grad else tensor.cpu().numpy()\n",
    "\n",
    "def evaluate_onnx(model_path, dataloader):\n",
    "    ort_session = ort.InferenceSession(model_path)\n",
    "    \n",
    "    counter = 0\n",
    "    correct_move = 0\n",
    "\n",
    "    maia_preds = []\n",
    "    with torch.no_grad():\n",
    "        for boards, labels, elos_self, elos_oppo, legal_moves, _, _ in tqdm.tqdm(dataloader):\n",
    "            # Convert torch tensors to numpy arrays\n",
    "            boards = to_numpy(boards)\n",
    "            elos_self = to_numpy(elos_self)\n",
    "            elos_oppo = to_numpy(elos_oppo)\n",
    "            legal_moves = to_numpy(legal_moves)\n",
    "\n",
    "            # Run the ONNX model\n",
    "            ort_inputs = {\n",
    "                'boards': boards,\n",
    "                'elo_self': elos_self,\n",
    "                'elo_oppo': elos_oppo\n",
    "            }\n",
    "            ort_outs = ort_session.run(None, ort_inputs)\n",
    "            logits_maia_legal = ort_outs[0] * legal_moves\n",
    "            preds = np.argmax(logits_maia_legal, axis=-1)\n",
    "            correct_move += (preds == to_numpy(labels)).sum().item()\n",
    "\n",
    "            counter += len(labels)\n",
    "            maia_preds.append(preds)\n",
    "\n",
    "            if counter > 10000:\n",
    "                break\n",
    "\n",
    "    return correct_move, counter, maia_preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def load_preprocessed_data(cache_path):\n",
    "    with open(cache_path, \"rb\") as f:\n",
    "        cache = pickle.load(f)\n",
    "    return cache[\"data\"], cache[\"game_count\"], cache[\"chunk_count\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/grace/u/dhkim2810/envs/maia/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configurations:\n",
      "\tseed: 42\n",
      "\tnum_workers: 0\n",
      "\tverbose: 1\n",
      "\tmax_ply: 300\n",
      "\tclock_threshold: 30\n",
      "\tuse_clock_filter: True\n",
      "\tchunk_size: 20000\n",
      "\tfrom_checkpoint: None\n",
      "\tmax_games_per_elo_range: 20\n",
      "\tbatch_size: 2048\n",
      "\tfirst_n_moves: 10\n",
      "\tlast_n_moves: 10\n",
      "\tdim_cnn: 256\n",
      "\tdim_vit: 1024\n",
      "\tnum_blocks_cnn: 5\n",
      "\tnum_blocks_vit: 2\n",
      "\tinput_channels: 18\n",
      "\tvit_length: 8\n",
      "\telo_dim: 128\n",
      "\tside_info: True\n",
      "\tvalue: True\n",
      "\tmax_depth: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 4/6816 [00:51<24:18:47, 12.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAIA2 Accuracy:  0.54521484375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "from eval_maia2 import parse_args\n",
    "from utils import seed_everything, get_all_possible_moves, create_elo_dict\n",
    "from data import read_cache_data, MAIA2Dataset\n",
    "\n",
    "cfg = parse_args(args=\"\")\n",
    "print(\"Configurations:\", flush=True)\n",
    "for arg in vars(cfg):\n",
    "\tprint(f\"\\t{arg}: {getattr(cfg, arg)}\", flush=True)\n",
    "seed_everything(cfg.seed)\n",
    "\n",
    "all_moves = get_all_possible_moves()\n",
    "all_moves_dict = {move: i for i, move in enumerate(all_moves)}\n",
    "elo_dict = create_elo_dict()\n",
    "\n",
    "val_paths = read_cache_data(\n",
    "\t\"/grace/u/dhkim2810/maia_gm/dataset/cache\",\n",
    "\t\"/grace/u/dhkim2810/maia_gm/file_list.pkl\",\n",
    "\tmode=\"validation\",\n",
    ")\n",
    "\n",
    "total = 0\n",
    "maia_correct = 0\n",
    "for val_file in val_paths[:1]:\n",
    "\tdata, game_count, chunk_count = load_preprocessed_data(val_file)\n",
    "\tdset = MAIA2Dataset(data, all_moves_dict, cfg)\n",
    "\tloader = torch.utils.data.DataLoader(\n",
    "\t\tdset, batch_size=cfg.batch_size, shuffle=False, num_workers=cfg.num_workers\n",
    "\t)\n",
    "\tcorrect, counter, preds = evaluate_onnx(onnx_model_path, loader)\n",
    "\ttotal += counter\n",
    "\tmaia_correct += correct\n",
    "maia2_acc = maia_correct / total\n",
    "print(\"MAIA2 Accuracy: \", maia2_acc, flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ONNX Model Performance : 55.29, 3.99s/batch, 2048 per batch\n",
      "UINT8 ONNX Model Performance : 54.52, 12.85s/batch, 2048 per batch\n"
     ]
    }
   ],
   "source": [
    "print(f\"ONNX Model Performance : 55.29, 3.99s/batch, 2048 moves per batch\")\n",
    "print(f\"UINT8 ONNX Model Performance : 54.52, 12.85s/batch, 2048 moves per batch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "- UINT8 is not ideal, INT8 is ideal but not supported\n",
    "- ONNX model can be improved using TensorRT(optimization kit)\n",
    "- MCTS search requires 75700+ inference for each move search\n",
    "\n",
    " - Leela is faster due to optimized neural network (c++)\n",
    " - When using GPU, we have around 1.2s/batch on Grace(single GPU)\n",
    "\n",
    "- hard to estimate but they say it can reach 20x speed \n",
    "- with onnx model, it's quite straightforward, but\n",
    "need additional implementation on data loading and preprocessing\n",
    "\n",
    "- Thats all"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.1.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
